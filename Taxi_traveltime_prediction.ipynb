{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Lab - 3 Taxi travel time\n",
    "\n",
    "# Robin Jamsahar, Mohamed Osman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#needed downloads\n",
    "#pip install fastparquet\n",
    "#pip install pyarrow"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "data/trips/yellow_tripdata_2019-01.parquet",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_6512/1976778693.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m#pd.show_versions()\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;31m#ytd = pd.read_parquet(\"data/trips/yellow_tripdata_2019-01.parquet\",engine=\"fastparquet\")\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[0mytd\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpq\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_table\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"data/trips/yellow_tripdata_2019-01.parquet\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto_pandas\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m \u001B[0mgtd\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpq\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_table\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"data/trips/green_tripdata_2019-01.parquet\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto_pandas\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mfhvtd\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpq\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_table\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"data/trips/fhv_tripdata_2019-01.parquet\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto_pandas\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pyarrow\\parquet.py\u001B[0m in \u001B[0;36mread_table\u001B[1;34m(source, columns, use_threads, metadata, use_pandas_metadata, memory_map, read_dictionary, filesystem, filters, buffer_size, partitioning, use_legacy_dataset, ignore_prefixes)\u001B[0m\n\u001B[0;32m   1698\u001B[0m             )\n\u001B[0;32m   1699\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1700\u001B[1;33m             dataset = _ParquetDatasetV2(\n\u001B[0m\u001B[0;32m   1701\u001B[0m                 \u001B[0msource\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1702\u001B[0m                 \u001B[0mfilesystem\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mfilesystem\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pyarrow\\parquet.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, path_or_paths, filesystem, filters, partitioning, read_dictionary, buffer_size, memory_map, ignore_prefixes, **kwargs)\u001B[0m\n\u001B[0;32m   1556\u001B[0m                 infer_dictionary=True)\n\u001B[0;32m   1557\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1558\u001B[1;33m         self._dataset = ds.dataset(path_or_paths, filesystem=filesystem,\n\u001B[0m\u001B[0;32m   1559\u001B[0m                                    \u001B[0mformat\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mparquet_format\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1560\u001B[0m                                    \u001B[0mpartitioning\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mpartitioning\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pyarrow\\dataset.py\u001B[0m in \u001B[0;36mdataset\u001B[1;34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001B[0m\n\u001B[0;32m    654\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    655\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0m_is_path_like\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msource\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 656\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0m_filesystem_dataset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msource\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    657\u001B[0m     \u001B[1;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msource\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mtuple\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    658\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mall\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_is_path_like\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0melem\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0melem\u001B[0m \u001B[1;32min\u001B[0m \u001B[0msource\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pyarrow\\dataset.py\u001B[0m in \u001B[0;36m_filesystem_dataset\u001B[1;34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001B[0m\n\u001B[0;32m    399\u001B[0m         \u001B[0mfs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpaths_or_selector\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_ensure_multiple_sources\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msource\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfilesystem\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    400\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 401\u001B[1;33m         \u001B[0mfs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpaths_or_selector\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_ensure_single_source\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msource\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfilesystem\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    402\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    403\u001B[0m     options = FileSystemFactoryOptions(\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pyarrow\\dataset.py\u001B[0m in \u001B[0;36m_ensure_single_source\u001B[1;34m(path, filesystem)\u001B[0m\n\u001B[0;32m    375\u001B[0m         \u001B[0mpaths_or_selector\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    376\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 377\u001B[1;33m         \u001B[1;32mraise\u001B[0m \u001B[0mFileNotFoundError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    378\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    379\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mfilesystem\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpaths_or_selector\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: data/trips/yellow_tripdata_2019-01.parquet"
     ]
    }
   ],
   "source": [
    "#pd.show_versions()\n",
    "#ytd = pd.read_parquet(\"data/trips/yellow_tripdata_2019-01.parquet\",engine=\"fastparquet\")\n",
    "ytd = pq.read_table(\"data/trips/yellow_tripdata_2019-01.parquet\").to_pandas()\n",
    "gtd = pq.read_table(\"data/trips/green_tripdata_2019-01.parquet\").to_pandas()\n",
    "fhvtd = pq.read_table(\"data/trips/fhv_tripdata_2019-01.parquet\").to_pandas()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ytd.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We only need to keep columns tpep_pickup_datetime, tpep_dropoff_datetime, trip_distance, PULocationID and DOLocationID"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ytd = ytd[ytd[\"fare_amount\"]>2.5]\n",
    "gtd = gtd[gtd[\"fare_amount\"]>2.5]\n",
    "fhvtd = fhvtd[fhvtd[\"fare_amount\"]>2.5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ytd = ytd[['tpep_pickup_datetime','tpep_dropoff_datetime','trip_distance','PULocationID','DOLocationID']]\n",
    "gtd = gtd[['lpep_pickup_datetime','lpep_dropoff_datetime','trip_distance','PULocationID','DOLocationID']]\n",
    "fhvtd = fhvtd[['pickup_datetime','dropoff_datetime','PULocationID','DOLocationID']]\n",
    "\n",
    "ytd.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To resume the data cleaning, we can convert the pickup and dropoff column to datatime. We can also remove rows where the pickuptime is greater than or equal to the dropoff time. We can also add a column 'trip_time' that says how long the trip lasted in seconds."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ytd['tpep_pickup_datetime'] = pd.to_datetime(ytd['tpep_pickup_datetime'])\n",
    "ytd['tpep_dropoff_datetime'] = pd.to_datetime(ytd['tpep_dropoff_datetime'])\n",
    "\n",
    "ytd = ytd.drop(ytd[ytd['tpep_pickup_datetime'] >= ytd['tpep_dropoff_datetime']].index)\n",
    "ytd['trip_time'] = (ytd['tpep_dropoff_datetime']-ytd['tpep_pickup_datetime']).dt.total_seconds()\n",
    "ytd['speed'] = (ytd[''])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ytd.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For convenience sake we can rename the pickup and dropoff columns to 'pickup_time' and 'dropoff_time' respectively. We can also only use time value in total seconds to make it easier to use in the model later."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ytd['tpep_pickup_datetime'] = (ytd['tpep_pickup_datetime']-ytd['tpep_pickup_datetime'].dt.normalize()).dt.total_seconds()\n",
    "ytd['tpep_dropoff_datetime'] = (ytd['tpep_dropoff_datetime']-ytd['tpep_dropoff_datetime'].dt.normalize()).dt.total_seconds()\n",
    "ytd.rename(columns = {'tpep_pickup_datetime':'pickup_time','tpep_dropoff_datetime':'dropoff_time'},inplace = True)\n",
    "ytd.info()\n",
    "ytd.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ytd = ytd[ytd.PULocationID < 264]\n",
    "ytd = ytd[ytd.DOLocationID < 264]\n",
    "ytd = ytd[ytd.trip_time != 0]\n",
    "ytd = ytd[ytd.trip_distance != 0.0]\n",
    "ytd=ytd[ytd.trip_time<20000]\n",
    "ytd.info()\n",
    "ytd.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now check if there are any missing values in our dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def missing_cols(df):\n",
    "    '''prints out columns with its amount of missing values'''\n",
    "    total = 0\n",
    "    for col in df.columns:\n",
    "        missing_vals = df[col].isnull().sum()\n",
    "        total += missing_vals\n",
    "        pct = df[col].isna().mean() * 100\n",
    "        if missing_vals != 0:\n",
    "            print(f\"{col} => {df[col].isnull().sum()},{round(pct,2)}%\")\n",
    "\n",
    "    if total == 0:\n",
    "        print(\"no missing values left\")\n",
    "\n",
    "missing_cols(ytd)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ytd.describe()\n",
    "corr = ytd[\"trip_time\"].corr(ytd[\"trip_distance\"])\n",
    "print(corr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "copy = ytd.copy()\n",
    "copy[\"pickup_time\"] = (ytd[(\"pickup_time\")] - ytd[(\"pickup_time\")].mean()) / ytd[(\"pickup_time\")].std()\n",
    "copy[\"trip_distance\"] = (ytd[(\"trip_distance\")] - ytd[(\"trip_distance\")].mean()) / ytd[(\"trip_distance\")].std()\n",
    "copy[\"PULocationID\"] = (ytd[(\"PULocationID\")] - ytd[(\"PULocationID\")].mean()) / ytd[(\"PULocationID\")].std()\n",
    "copy[\"DOLocationID\"] = (ytd[(\"DOLocationID\")] - ytd[(\"DOLocationID\")].mean()) / ytd[(\"DOLocationID\")].std()\n",
    "\n",
    "\n",
    "#code = diabetes[\"Code\"]\n",
    "copy[\"trip_time\"] = (ytd[(\"trip_time\")] - ytd[(\"trip_time\")].mean()) / ytd[(\"trip_time\")].std()\n",
    "\n",
    "X = np.asarray(copy[[\"pickup_time\",\"trip_distance\",\"PULocationID\",\"DOLocationID\"]])\n",
    "Y = np.asarray(copy[\"trip_time\"])\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.33,random_state = 1)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()\n",
    "lm = lm.fit(X_train, Y_train)\n",
    "Y_hat = lm.predict(X_test)\n",
    "print(\"Linear Regression:\")\n",
    "print(\"Accuracy: \",r2_score(Y_test, Y_hat))\n",
    "print(\"Error: \",mean_squared_error(Y_test, Y_hat))\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# f = RandomForestRegressor()\n",
    "# f = f.fit(X_train, Y_train)\n",
    "# Y_hat = t.predict(X_test)\n",
    "# print(\"Random Forest Regressor:\")\n",
    "# print(\"Accuracy: \",r2_score(Y_test, Y_hat))\n",
    "# print(\"Error: \",mean_squared_error(Y_test, Y_hat))\n",
    "\n",
    "\n",
    "\n",
    "# X = ytd[['trip_distance','pickup_time']]\n",
    "# Y = ytd['trip_time']\n",
    "# X = X.values.reshape(len(X), 2)\n",
    "# Y = Y.values.reshape(len(Y), 1)\n",
    "\n",
    "# X_train = X[:int(0.8*len(X))]\n",
    "# X_test = X[int(0.8*len(X)):]\n",
    "\n",
    "# Y_train = Y[:int(0.8*len(Y))]\n",
    "# Y_test = Y[int(0.8*len(Y)):]\n",
    "\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# t = DecisionTreeRegressor()\n",
    "# t = t.fit(X_train, Y_train)\n",
    "# Y_hat = t.predict(X_test)\n",
    "# print(r2_score(Y_test, Y_hat))\n",
    "\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# lm = LinearRegression()\n",
    "# lm = lm.fit(X_train, Y_train)\n",
    "# Y_hat = lm.predict(X_test)\n",
    "# print(r2_score(Y_test, Y_hat))\n",
    "# print(mean_squared_error(Y_test, Y_hat))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}